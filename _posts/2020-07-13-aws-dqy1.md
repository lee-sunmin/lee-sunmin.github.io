---
title: "[bigdata] Big Data on AWS - day 1"
categories:
  - bigdata
tags:
  - aws
  - bigdata
---



S3DistCP

- S3DistCP 압축 지원
데이터를 지정한 압축 포잿으로 지정해서 저장 할 수 있음.



#### Apache Sqoop을 사용한 데이터 전송
HDFS(Hadoop Distribution File System)에서 RDB로 혹은 양방향 전송 가능하다.  


#### 데이터 전송 옵션

VPN 연결이 가능하다.  
사내 데이터 센터에 ipsec 이 있으면 AWS 를 프라이빗하게 쓸 수 있는 VPN 연결 제공해준다. 

성능이 보장되지 않는다는 단점이 있다.  
해결을 위해 AWS Direct Connect 라는 서비스가 물리적 회선을 제공해준다.  
장점은 안정성.  회선 분리하고 싶으면 파트너 통해서 분리도 가능  
AWS에서 밖으로 데이터를 내보낼 때 쓰는 비용이 그냥 인터넷을 사용하는 것보다 적기 때문에 비용절감.  

AWS 비용 발생 3가지  
1) 컴퓨팅 - CPU 사용하는 작업  
2) 저장 - S3와 같은 저장장소를 얼마나 사용 할 것인가  
3) 데이터 전송 - AWS로 데이터를 보내는 것은 공짜고, 외부로 데이터를 보내는 것에 대해서만 비용이 발생한다.  

#### ** 시험문제 포인트 **  
데이터 전송 옵션 !  
  
  
마이그레이션을 몇일에 진행 하는데 몇시 안에 끝나야 한다(시간제약)
데이터를 옮겨야 하는데 다음중에 어떤 것을 사용할 것인가?

1) 시간성 제약, 단발성  
단기간에 데이터 왕창 올려아 한다 - > 스노우볼. 직접 하드웨어를 받아서 데이터를 넣고 보내는 방식  
2) 실시간도 허용이 되는데, ***지속적***으로 받아야 한다 - > 다이렉트 커넥트  
3) 좀 늦어져도 상관 없는데, ***시간에 대해서 아무 엄격하지 않다*** - > 그냥 인터넷을 통과하는 s3 멀티파트 업로드나, vpn 사용 가능

어떤 때에 무얼 쓰느냐? 를 적절하게 사용해야 함.


데이터 마이그 할 수 있는 솔류션(red shift)  
파일로 옮기면(s3)  
S3 -> EMR에서 제공하는 HDFS

S3는 오브젝트 스토리, 웹 하드처럼 사용.  
내구성이 엄청 좋다. 1년동안 데이터가 깨지지 않을 확률이 9가 11번 있는 99.999x  
여러개의 복제를 만들어서 복제본들이 서로서로 깨지지 않았는지 체크를 함으로써 손상이 된 복제본이 있으면 자동으로 교체하는 방식 사용해서 내구성 유지하고 있음.  
여기 s3 데이터를 넣으면 깨질 일이 없다고 보면 된다.  
5T 1만개를 천만년동안 보관했을 때 하나가 깨질랑말랑할 확률이다.  


****

#### AWS 데이터 전송 솔루션  

Snowball Edge - 한 개당 100TB  
Snowmobile - 컨테이너 하나당 PB (데이터 센터 통째로 옮겨오는데 사용)  


#### AWS IoT  
사물 인터넷에 사용되는 디바이스를 관리 할 수 있는 기능 제공  
여러 디바이스로부터 데이터 수집, 저장, 분석(필터링) 가능  

Lambda, kinesis, s3, machine learning, dynamo db와 저장 및 통합해서 사용하는 것이 가능  

작동 방식  (교재 참고)  

#### AWS IoT의 기능 : 규칙 엔진  
자체적인 문법(where ..)를 가지고 있다. 즉, 필터링 할 수 있는 문법 제공  
이걸 통해서 어떤 값이 몇분 이상 지속됐다 -> 다른 시스템으로 메시지 이전 가능(가능한 범위는 교재 참고)  


### Module 3
#### Kinesis  
데이터가 생산 될 때마다 보내는 방식 : 스트리밍(Streaming)  
스트리밍과 배치 사이 - 마이크로 배치(어느정도 스트리밍)  

배치 처리 : 유한 데이터를 지원하도록 설계  
어느정도 데이터를 모았다가 전송하기 때문에 압축을 하거나 포맷을 바꿔서 효율적으로 통신 할 수 있음.  
스트림 처리 : 연속 데이터를 지원하도록 설계  
최대한 빨리 처리하도록 할 수 있음. 단순히 시간만 빠른게 아니고 너무 데이터의 양이 많아서 배치로 처리가 안되는 경우 스트림 처리를 통해 그때그때 쌓인 데이터를 전송하기도 한다.   
  
#### 스트리밍 데이터를 솔루션에 사용해야 하는 이유
1. 수집과 처리의 분리
2. 복수 스트림의 동시 수집
3. 메세지의 순서 유지- fifo (kafka와 유사)  
SQS는 순서의 보장이 안된다.  
4. 병렬 소비  

#### 스트림 처리 어플리케이션 특징
(강의 자료)  
lambda 아키텍처 구현 : 상태를 공유하지 않은 상태에서 전달, 공유 ?  

#### 빅데이터 스트림에서 데이터 윈도우 작업  
1. 고정 : 고정크기의 임시길이
2. 변동 : 생산되는 양이 들쑥날쑥 할 때, 특정 양이 모일 때까지 기다렸다가 분할하는 방식으로 할 수 있음  


### Amazon Kinesis
** 자격증 포인트 **  
실시간 어쩌구.. **실시간**이라는 단어가 나오면 거의 kinesis ***  
지금은 managed kafka , kinesis 두 가지 선택지  

**1회 전달 보장 - Kafka**  
kafka 단점은 비용이 kinesis보다는 더 발생한다는 단점이 있음.(리소스 많이)  
  
**kinesis는 순서 보장, 전달 보장하지만 1회 전달이 보장되지 않는다**  
데이터가 나갈 때 동시에 2개를 가져 갈 수 있음.  


****
kinesis 4가지(강의자료 참고)  

kinesis data streams는 단순 데이터 저장  
데이터를 수집 및 스트리밍 하여 순서대로 실시간 처리  

### Data Firehose
스트리밍 데이터를 캡쳐하고 변화하여 아래 4가지로 로드 가능   
kinesis data analytics, s3, redshift, elasticsearch service  
  
- 데이터를 로드하기 전에 배치 처리, 압축 및 암호화 할 수 있다  
orc, parque  


*추가 설명-orc, parque*  
데이터를 column으로 저장을 해서, select 할 때 특정 column만 읽어오고 싶을 때 i/o의 부담이 줄어든다.  
그리고 여러 column을 선택했을 때, 비슷한 포맷?일 경우가 많아서 압축에 굉장히 용이하다.(압축효율 향상)  

#### 작동 방식 및 이점

스트리밍 데이터를 캡쳐하여 Firehose Rest API 사용하여 데이터 제출  


이점  
1. 지속 관리 불필요  
2. 지연 시간이 거의 없는(x 60초정도) 준실시간 응답  
3. 사용 편의성

개념 (강의자료 참고)
Firehose 전송 스트림  
- Firehose 기반 엔티티
- 샤드 프로비저닝 없음  
- 파티션 키 없음

**데이터 생산자**  
데이터 생산자는 전송 스트림에 레코드를 전송하는데,
키네시스에서 제공하는 생산자 라이브러리를 사용하거나 로그 수집기나 데이터 수집 툴에서 (flume,..) 키네시스로 데이터를 보낼 수 있는 기능 제공하니 그거 사용한다.  

### Kinesis Data Streams  
데이터 저장 + 처리(포맷 변경, 필터링)  

firehose보다 훨씬 높은 준실시간 성능(수초 이내)  

kinesis 생산자가(File로 떨구거나 log 수집기 이용해서 데이터 밀어넣음) -> kinesis streams에 넣고 -> kinesis 소비자(kinesis 소비자 library 사용해서 개발 많이 함)가 생성된 데이터 사용 -> 붙여서 쓸 수 있는 서비스의 제약은 거의 없다  

#### Kinesis Data Streams를 통한 데이터 이전 방법
(강의자료 참고)  
map reduce 과정과 동일  

#### Kinesis Data Streams 어플리케이션 구축 - 어렵당 .. 
(강의자료 참고)  
EC2 인스턴스의 갯수를 늘렸다가 줄였다가 할 수 있음-오토스케일링(Auto Scaling)  

#### 커넥터 라이브러리
이걸 사용해서 키네시스에 쌓인 데이터를 여러 서비스로 넣을 수 있음  

firehose는 S3 Redshift EMR ES 4개정도만 지원했는데  
이건 좀 더 다양하게 지원한다.

단, 오토 스케일링을 통해 용량이 확장 될 수 있도록 설정을 해줘야 함  


#### firehose vs streams
** 시험 포인트 **  
streams - **거의 실시간, 1초 미만**  
firehose - 동기화 되는데 1분 이상 걸려도 상관이 없다.  
붙여도 되는거 S3, RedShift, ES  
대신 관리가 전혀 필요없다.
******  

### Kinesis Video Streams

text 기반이 아닌 binary 기반 데이터 사용  
영상분석 서비스와 연결해서 사용 할 수 있는 기능 가지고 있음  

amazon recognition 같은 AI 서비스와 연결해서 사용 할 수 있다  


### Kinesis Analyrics(스트리밍 분석)  

지금까진 받고 처리해서 넘겨주는 것 까지 했는데.  
필터된 애들만 분류 하고싶을 때 사용 할 수 있는 서비스  

가장 큰 특징은 SQL 통해서 데이터 필터링 가능  
실시간성(1초 미만), 데이터 처리량에 따라 탄력적 확장  

#### 스트리밍 소스에 연결

- firehose, streams 둘 다 가능함  
- json, csv, 변수 열, 비구조화 텍스트 -> 결국 text 데이터만 대상으로 할 수 있다!  
- 자동으로 스키마 추론 - 테이블 형태로 access 할 수 있도록 만들어주는 기능  


#### SQL을 사용하여 스트리밍 데이터 처리
**최소 한 번** 처리 의미 체계 - 하나 넣었는데 두개 나올수도 있다.  
1회 전달이 보장이 안된다.. !  

#### 처리된 데이터를 연속적으로 전송
처리와 전송을 분리 할 수 있음  

### 기타 스트림 처리 어플리케이션  

1. Apache Spark Streaming  
스트리밍 데이터를 1초 미만의 마이크로배치로 분할하여 처리  
다양한 변환, 다양한 데이터 소스, 다양한 데이터 저장소 선택하여 사용 가능  
kafka, flume, twitter, kinesis, s3, tcp/ip 로부터 데이터 읽을 수 있음  

2. Apache Kafka  
실시간 성으로 들어오는 데이터를 안전하게 두고 **순서를 지키면서 1회 전달을 보장**해 준다

3. 그 외  
EC2 또는 EMR에 설치 가능한 도구(자바 런타임 환경)  
Flume, Storm, Samza, Flink  


#### 기타
HUGI(Hurry up Get I..?)  
인스턴스 한대가 10시간 동안 돌아야 끝나는 작업  
병렬이 가능하면 인스턴스를 10대 사용해서 1시간만에 완료  
가격은 양쪽이 똑같다. 시간이 줄었으니까?????  

장기 분석은 S3, 단기 분석은 RedShift


#### 어떤 스트림 스토리지를 사용해야 하는가?
(강의자료 참고)  
** 자격증 포인트 **  
가장 느린애 - SQS(표준,FIFO)  
실시간성이 잘 보장 안되는애 - firehose  
제일 빨리 도착하는애 - Kinesis streams, Kafka  

(사진 참고)  
****

질문  
1. 어느정도 처리?
2. 확장 축소 가능?
3. 내결함성?
4. 전송 보장?
5. 어떤 프로그래밍 언어 지원?
6. 누가 관리?(관리에 대한 부담감)  

#### 모듈 복습 문제

1. kinesis data streams 샤드  
샤드는 내부적으로 인스턴스와 동일  
고정된 처리용량을 지닌 처리 유닛  
읽기는 초당 1mb., 쓰기는 2mb 라는 고정된 처리용량 지니고 있음  

2. kinesis 이점 두 가지  
높은 처리량, aws 서비스 통합, 사용 및 관리의 용이성, 저비용, 준실시간 성능

3. Kinesis는 대용량 데이터 파일의 스트리밍에 적합한가?  
거짓!  
대용량 데이터 파일 스트리밍...?  
파일 덩어리 하나가 큰거 처리는 적합하지 않고 여러개의 작은 파일 여러개를 처리하는 것에는 적합하다.  

자격증 시험 문제는 절대 이런식으로 나오진 않아요 ^^;  
다만, 파일이 2mb 안쪽의 파일이 여러개 생성됐다. 어떻게 처리할까? 이런건 물어 볼 수 있음.  



### 실습  
analytics 이용해서 필터링 하고, firehose 써서 s3 버킷에 넣는거  


### 모듈 4

#### 클라우드 데이터베이스 및 스토리지 티어
어플리케이션 처리와 저장을 분리하는 것이 중요하다  

안티패턴
RDBMS 하나로 모든것을 다 처리하는 것은 옛날방식  
NoSQL DB 를 쓰려면 또 DBA를 둬야하나..? > 부담  
지금은 클라우드 관리형서비스를 사용하여 DBA 가 하는 대부분의 업무를 클라우드 업체에서 해주기 때문에 용이하다(백업, 패치, 용량, fail over)  

#### AWS와 사용 가능한 스토리지 솔루션
(교재 참고)  
EMR은 안에 하둡HDFS가 있어서 포함  

#### 스토리지 솔루션 개념 - 데이터 레이크
중앙 집중식으로 방대한 볼륨을 가지고 있는 데이터 저장장소를 만들고 유지  
중앙 집중식 저장소에 데이터를 저장하고 분석 할 수 있기 때문에 여러 솔루션들이 한 군데에만 접근하면 된다.  
일단 원시데이터를 저장해놓고 안에서 정제작업 등을 수행 할 수 있다.  
스토리지와 컴퓨팅 계층을 분리 할 수 있고, 임시 분석 수행에도 용이하다.  
-> S3

#### 데이터 레이크 vs 데이터 웨어하우스  
(교재 참고)  
데이터 웨어하우스는 스키마가 정해져 있는 경우에 사용.  
구조화 데이터만, 쓰기용 스키마(사전 정의된 스키마)  
</br>
데이터 레이크는 우선 저장해놓고 나중에 내용물 까봄  
읽기용 스키마(사전 정의되지 않음)  
세분화 정도가 낮음  

로그데이터는 각자 개발자가 편의를 위해서 넣어놓는데 두 가지 의미에서 분석 할 필요가 생긴다  
- 있어서는 안되는 데이터가 있는걸 체크하기 위해(개인정보성)  
- 에러 확인?

로그데이터는 포맷도 제멋대로라서 좀 까다롭다.  


#### 권장 환경
SQL문법으로 웬만한거 다 사용 가능  
S3를 데이터레이크로 연결해서 쓸 수 있는 애들이 EMR, Kinesis, Data 파이프라인, RedShift, DynamoDB, RDS, ...  

#### 데이터 레이크에 S3를 사용해야 하는 이유  
** 자격증 **

중요한 정보라서 상중하(??)가 기본적으로 되어있는 저장장소에 저장해야 한다.  
S3는 기본적으로 세 곳 이상의 공간에 copy를 하기 때문에 내구성이 좋고, 가용성도 좋다. 특히 병렬로 처리하는데 있어서 굉장히 특화되어 있음.  
각 언어별로 잘 정의된 SDK가 있고, 확장성도 뛰어나다.  

******


### Amazon Glacier
- 객체 스토리지  
- 읽을 일이 거의 없는 장기간 보관 데이터에만 사용해야 한다(엑세스가 빈번하지 않은 데이터에 최적화)  

### 스토리지 솔루션 개념
**NoSQL 데이터베이스**  
- 스키마가 고정되어 있지 않다.(자유롭게 변화 가능)
- 최종 일관성
- 분산형(여러개의 머신을 분산해서 수행 가능)
- 수평 확장성(인스턴스의 갯수를 늘리거나 줄임에 따라 처리 용량 늘리거나 줄일 수 있음) : 인스턴스의 갯수를 늘리거나 줄이는 방식으로 확장  

#### SQL vs NoSQL
(교재 참고)

전통적 RDBMS  
NoSQL - 컴퓨팅 최적화, 빠른 속도  

#### NoSQL 데이터베이스 유형
1. column(열) 기반 데이터 스토어 
2. 문서 데이터 스토어 -> JSON  
반구조화 데이터(JSON), 몽고디비, cassandra, ..  
3. 그래프 데이터 스토어
4. 인메모리 키-값 스토어

#### Amazon DynamoDB
- Key-value 인데 value에 JSON이 들어가는 형태(테이블이 고정 스키마를 가지지 않음)  
key는 JSON에 포함되어 있는 특정 속성  
- 지연 시간 10밀리초 미만의 성능 보장(돈을 내고 성능을 산다 ㅎㅎ^^)
- SD를 저장장소로 사용하는데, Dynamo DB를 움직이기 위해 전용으로 설계된 SSD 사용  
- 높은 내구성 및 확장성(세 군데 이상의 시설에 삼중화로 복제 되어서 사용되기 때문에 내결함성이 뛰어나다)  
- EMR과 통합
- 다양한 데이터 유형  
- 항목 및 속성  
column - attribute  
검색을 위해 사용 할 수 있는 column - partition  
검색 된 결과에 대해 정렬 할 수 있는 column - 정렬 키  
로컬 보조 인덱스  
GSI(Global Secondary Index) : 주 검색키가 아닌 다른 키로 검색하고 싶을 때 사용 할 수 있음  
- 전역 테이블 - 데이터를 여러 region에 복제함으로써 그 region에 있는 사람들이 좀 더 빠르게 사용 할 수 있도록 한다  
문제가 발생 했을 때, 나머지 region을 통해 복구도 할 수 있다  
- 인덱싱 및 분할  
** 시험 포인트(정말 잘나와) **   
로컬 보조 인덱스(LSI) - 파티션에 대해서 Local이라는 건데  
다른 attribute를 가지고 sort 하고 싶을 때 사용  
글로벌 보조 인덱스(GSI) - 다른 파티션 key로 검색을 하고 싶을 때 사용
******
- DAX(DynamoDB Accelerator)  
매번 비슷한 패턴의 쿼리가 들어오면, 인메모리 캐시를 사용(DAX)  
어플리케이션에서는 DB가 2개 인 것 처럼 보인다  
캐시다 보니까 최신 데이터가 없을 순 있지만 굉장히 빠르게 작동된다  

#### NoSQL 그래프 데이터베이스 
1. Amazon Neptune  
aws에 최적화된 스토리지 서비스 사용  
neptune을 위한 그래프 엔진이 있음.
2. JanusGraph  

#### 인메모리 키-값 스토어
1. ElasticCash  
완전 관리형, Memcached 및 Redis  


### Amazon Redishift
오라클과 호환성이 높은 SQL로 쿼리 날릴 수 있음  
column 기반의 스토리지 사용  

### Amazon RDS
Amazon Aurora - 구조적으로는 Oracle RAC 와 흡사. 스토리지 레이어를 별도로 분리해서 구현. 쿼리는 MySQL 그대로 사용. 병렬 쿼리에 강함.  

Sqoop 사용해서 EMR 데이터를 RDS로 직접 출력 할 수 있으며, 반대도 가능


### 비구조화 데이터 스토어(Amazon EMR(HDFS)-EMRFS)
EC2에 직접 하드웨어로 붙어있는 스토리지라서 성능 아주 좋음  

S3와 연결해서 사용 가능  
S3나 Dynamo DB에 있는 데이터를 EMR이 직접 읽어오는 것이 가능하다.  

### 데이터 스토어 선택 시 고려 사항
(교재 참고)  
#### 데이터 스토어 선택 : 비용 고려 사항  
S3 vs DynamoDB  

1. 시나리오 1  
쓰기 300회/초  
2,048바이트 객체 크기  
저장공간 1,483GB/월  

(1) S3 - 스토리지 비용은 저렴함. 근데 쓰기가 많기 때문에 put 비용이 많이 발생한다.  
(2) DynamoDB - 스토리지 비용은 비쌈. 거의 10배  
그런데 put 요청에 대한 발생(request)가 훨씬 저렴해서 총 금액은 얘가 더 저렴  
즉, 작은 데이터를 여러번 읽고 쓰는건 DynamoDB가 좋다. S3의 1/6  


2. 시나리오 2  
쓰기 300회/초  
32,768바이트 객체 크기
저장공간 23,730GB/월  

(1) S3 - 이전과 유사  
(2) DynamoDB - 금액이 급격하게 증가한다. 필요로 하는 처리량이 아까랑 많이 차이가 난다. 왜냐하면 데이터 용량이 커지면 필요로 하는 처리량이 늘어나기 때문임  
즉, S3보다 2배정도의 비율이 발생  

3. 쓰기가 아닌 조회가 상당히 자주 일어나는 경우  
S3와 DynamoDB를 같이 사용  
조회는 Dynamo 접근은 S3(데이터는 S3 검색은 DynamoDB)  

특히, S3는 key/value 스토어라서 검색을 한다던지 하는게 잘 안된다. 특정 이름을 지정해서 가지고 오는 것만 가능한데 이걸 빠르게 하기 위해서 DynamoDB의 도큐먼트 디비 특징을 사용해서 인덱싱을 하고, 쿼리를 해서 상당히 빠르게 동작 가능  



#### 모듈 복습  
1. Dynamo DB 두가지 이유  
:  빠르고 확장가능, 보안에도 탁월 -> 스키마 없는 방식으로 작업, Hive 호환  
2. 지연 시간이 가장 짧은 것  
Dytnamo - Redshift - S3 - Glacier
3. **고도로 구조화**된 데이터에 가장 적합한 스토리지  
RDS


### 모듈 5
#### Amazon Athena  
대용량의 EMR 클러스터가 미리 워밍업이 되어있고, 쿼리를 하는 순간에 S3안에 있는 데이터를 읽어서 결과를 뽑아내기 때문에 상당히 속도가 빠르다.  
  
데이터 제어 언어(DCL) - 하이브 메뉴얼 참고하면 좋음  
  
redshift - 정형화된 데이터 분석 사용  
emr - 비정형화된 데이터 분석 사용  


1. Athena 임시쿼리 이점  
인프라 유지보수 필요 없다, 값싸고 빠르다, S3를 SQL로 분석 가능  
2. Parquet, orc  
3. Athena 카탈로그 스토어  

실습  
90GB 데이터의 카운트나 그런거 뽑아내는데 13초 걸림  
13GB의 S3 zip 파일로 묶인 데이터 쿼리 21초 걸림 - 용량은 85% 줄었음  






